{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfsFMFBOkMjgGH2cnKxcrl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaetanAm/Master-1-Machine-Learning/blob/main/notebooks/TD4bis/Exercice_2_3_AMIENS_AMADIEU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCICE 2 : data undersampling"
      ],
      "metadata": {
        "id": "FKAajBYoypid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the dataset in your notebook and check if there are any missing values"
      ],
      "metadata": {
        "id": "Vvd5s6GK70Mi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "gXLF2BLlyn1u",
        "outputId": "472ecd65-a09e-481e-ecc1-1158ef406397"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-02b757a0-d8f5-4476-b2c2-c2e187b7c9c6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-02b757a0-d8f5-4476-b2c2-c2e187b7c9c6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving creditcard.csv to creditcard.csv\n"
          ]
        }
      ],
      "source": [
        "# Importer les bibliothèques nécessaires\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "# Télécharger le fichier localement\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le dataset à partir du fichier téléchargé\n",
        "data = pd.read_csv(list(uploaded.keys())[0])\n",
        "\n",
        "# Vérifier les premières lignes\n",
        "print(\"Aperçu du dataset :\")\n",
        "print(data.head())\n",
        "\n",
        "# Vérifier les valeurs manquantes\n",
        "print(\"\\nValeurs manquantes dans chaque colonne :\")\n",
        "print(data.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULfdnU2u6mw_",
        "outputId": "99a7211a-db9c-48f7-8c4d-f10ce9d168ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aperçu du dataset :\n",
            "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
            "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
            "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
            "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
            "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
            "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
            "\n",
            "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
            "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
            "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
            "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
            "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
            "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
            "\n",
            "        V26       V27       V28  Amount  Class  \n",
            "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
            "1  0.125895 -0.008983  0.014724    2.69      0  \n",
            "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
            "3 -0.221929  0.062723  0.061458  123.50      0  \n",
            "4  0.502292  0.219422  0.215153   69.99      0  \n",
            "\n",
            "[5 rows x 31 columns]\n",
            "\n",
            "Valeurs manquantes dans chaque colonne :\n",
            "Time      0\n",
            "V1        0\n",
            "V2        0\n",
            "V3        0\n",
            "V4        0\n",
            "V5        0\n",
            "V6        0\n",
            "V7        0\n",
            "V8        0\n",
            "V9        0\n",
            "V10       0\n",
            "V11       0\n",
            "V12       0\n",
            "V13       0\n",
            "V14       0\n",
            "V15       0\n",
            "V16       0\n",
            "V17       0\n",
            "V18       0\n",
            "V19       0\n",
            "V20       0\n",
            "V21       0\n",
            "V22       0\n",
            "V23       0\n",
            "V24       0\n",
            "V25       0\n",
            "V26       0\n",
            "V27       0\n",
            "V28       0\n",
            "Amount    0\n",
            "Class     0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No missing value, we have a proper dataset"
      ],
      "metadata": {
        "id": "B3wNG1LQ78eB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create variables X and y, where X will contain the 30 features and y will be the target “Class”"
      ],
      "metadata": {
        "id": "NjYgT2268Njb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Définir X (caractéristiques) et y (cible)\n",
        "X = data.iloc[:, :-1]  # Toutes les colonnes sauf la dernière (Class)\n",
        "y = data['Class']      # Colonne cible\n",
        "\n",
        "print(f\"\\nDimensions des données : X = {X.shape}, y = {y.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WCezXj26tdl",
        "outputId": "39326d88-88bf-4783-c766-922d0fe101c1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dimensions des données : X = (284807, 30), y = (284807,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the data into a training set and a test set with a test set of 20%.\n",
        "You need to import train_test_split from sklearn.model selection. By setting the stratify argument to data[”Class”], you’re instructing the\n",
        "train_test_split function to preserve the class proportions in both the training and testing sets. This ensures\n",
        "that both classes are represented in a balanced manner. You can check the number of values present in a set\n",
        "with the method value_counts() to ensure that both classes are well represented in y_train and y_test."
      ],
      "metadata": {
        "id": "xR8qBJ-B8jM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Diviser les données en train et test avec stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Vérifier la répartition des classes\n",
        "print(\"\\nRépartition des classes dans y_train :\")\n",
        "print(y_train.value_counts(normalize=True))\n",
        "print(\"\\nRépartition des classes dans y_test :\")\n",
        "print(y_test.value_counts(normalize=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrGirZdH6w2o",
        "outputId": "81426465-7d7c-4346-d499-fa70c6f15f87"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Répartition des classes dans y_train :\n",
            "Class\n",
            "0    0.998271\n",
            "1    0.001729\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Répartition des classes dans y_test :\n",
            "Class\n",
            "0    0.99828\n",
            "1    0.00172\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette répartition initiale démontre un fort déséquilibre des classes. Le sous-échantillonnage permet de travailler sur un dataset équilibré pour entraîner le modèle."
      ],
      "metadata": {
        "id": "mm8ptadaI_z2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create your undersampled data using the resample method from sklearn.utils to keep the same number of samples for each target “Class”.\n",
        "Be careful : resampling should be applied only to the training set to avoid\n",
        "data leakage.\n",
        "\n",
        "Data leakage occurs when information from the testing set is inadvertently used to train the model.\n",
        "\n",
        "This can lead to overfitting and inflated performance metrics, as the model becomes too tailored to the specific examples in the testing set.\n"
      ],
      "metadata": {
        "id": "KQ6r5vFOJFew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "# Combiner X_train et y_train pour faciliter la manipulation\n",
        "train_data = pd.concat([X_train, y_train], axis=1)\n",
        "\n",
        "# Séparer les classes majoritaire et minoritaire\n",
        "majority_class = train_data[train_data['Class'] == 0]\n",
        "minority_class = train_data[train_data['Class'] == 1]\n",
        "\n",
        "# Sous-échantillonnage de la classe majoritaire\n",
        "majority_downsampled = resample(\n",
        "    majority_class,\n",
        "    replace=False,                # Échantillonnage sans remplacement\n",
        "    n_samples=len(minority_class),  # Même nombre que la classe minoritaire\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Combiner les deux classes\n",
        "undersampled_data = pd.concat([majority_downsampled, minority_class])\n",
        "\n",
        "# Répartition après sous-échantillonnage\n",
        "print(\"\\nRépartition des classes après sous-échantillonnage :\")\n",
        "print(undersampled_data['Class'].value_counts())\n",
        "\n",
        "# Redéfinir X_train et y_train\n",
        "X_train_undersampled = undersampled_data.iloc[:, :-1]\n",
        "y_train_undersampled = undersampled_data['Class']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET4iIA3D6zDJ",
        "outputId": "d46e0330-f0cc-4937-c47e-9ae180e9432e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Répartition des classes après sous-échantillonnage :\n",
            "Class\n",
            "0    394\n",
            "1    394\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les classes sont équilibrées : 394 exemples dans chaque classe."
      ],
      "metadata": {
        "id": "k6dTnD9IJq3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train a Logistic Regression model on the original data and another Logistic Regression model on the undersampled data.\n",
        "We will then compare the performance of these two models to see if our undersampled data allowed better results."
      ],
      "metadata": {
        "id": "CvyDKu5SJ9Xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Modèle sur les données originales\n",
        "model_original = LogisticRegression(max_iter=200, random_state=42)\n",
        "model_original.fit(X_train, y_train)\n",
        "y_pred_original = model_original.predict(X_test)\n",
        "\n",
        "print(\"\\nRapport de classification (données originales) :\")\n",
        "print(classification_report(y_test, y_pred_original))\n",
        "\n",
        "# Modèle sur les données sous-échantillonnées\n",
        "model_undersampled = LogisticRegression(max_iter=200, random_state=42)\n",
        "model_undersampled.fit(X_train_undersampled, y_train_undersampled)\n",
        "y_pred_undersampled = model_undersampled.predict(X_test)\n",
        "\n",
        "print(\"\\nRapport de classification (données sous-échantillonnées) :\")\n",
        "print(classification_report(y_test, y_pred_undersampled))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNwoIkE161DO",
        "outputId": "d1aa000b-821c-43b8-9100-6e6587d7c7fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Rapport de classification (données originales) :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56864\n",
            "           1       0.76      0.69      0.72        98\n",
            "\n",
            "    accuracy                           1.00     56962\n",
            "   macro avg       0.88      0.85      0.86     56962\n",
            "weighted avg       1.00      1.00      1.00     56962\n",
            "\n",
            "\n",
            "Rapport de classification (données sous-échantillonnées) :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.98     56864\n",
            "           1       0.03      0.92      0.06        98\n",
            "\n",
            "    accuracy                           0.95     56962\n",
            "   macro avg       0.52      0.94      0.52     56962\n",
            "weighted avg       1.00      0.95      0.97     56962\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate both models by comparing the accuracy, precision, recall and F1-score metrics."
      ],
      "metadata": {
        "id": "o5RcgXQJKO2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Précision globale (accuracy) :\n",
        "\n",
        "Le modèle original a une précision parfaite, mais cela est trompeur dans un contexte de données déséquilibrées.\n",
        "Le modèle sous-échantillonné a une précision légèrement inférieure à 95 %, mais il offre une meilleure détection des fraudes.\n",
        "\n",
        "### Rappel (classe fraude) :\n",
        "\n",
        "Le rappel passe de 0.69 à 0.92, ce qui est crucial pour capturer un maximum de fraudes.\n",
        "\n",
        "### F1-score (classe fraude) :\n",
        "\n",
        "Bien que le rappel augmente avec le sous-échantillonnage, la précision faible tire le F1-score vers le bas."
      ],
      "metadata": {
        "id": "eSv1HOobKSEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCICE 3 : Random Oversampling Technique\n"
      ],
      "metadata": {
        "id": "5xY5hjw3NrJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple random oversampling involves randomly duplicating samples from the minority class to balance the\n",
        "dataset.\n",
        "\n",
        "When to Use:\n",
        "\n",
        "  - Moderate unbalanced class: When the majority class doesn’t significantly outnumber the minority class.\n",
        "\n",
        "  - Small datasets: When the dataset is relatively small and increasing the sample size can improve model performance.\n"
      ],
      "metadata": {
        "id": "nT2y_-V4NvUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first steps are the same: load the dataset, check missing values, train/test split data. Let’s continue to the\n",
        "part where we check if our sets possess all possible values with value counts()."
      ],
      "metadata": {
        "id": "XiEnQw-_OTxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create your oversampled data using the resample method from sklearn.utils to keep the same number ofsamples for each value of the target “Class”.\n"
      ],
      "metadata": {
        "id": "0bkW_pHmRfCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "# Combiner X_train et y_train pour faciliter la manipulation\n",
        "train_data = pd.concat([X_train, y_train], axis=1)\n",
        "\n",
        "# Séparer les classes majoritaire et minoritaire\n",
        "majority_class = train_data[train_data['Class'] == 0]\n",
        "minority_class = train_data[train_data['Class'] == 1]\n",
        "\n",
        "# Oversampling de la classe minoritaire\n",
        "minority_oversampled = resample(\n",
        "    minority_class,\n",
        "    replace=True,                 # Oversampling avec remplacement\n",
        "    n_samples=len(majority_class), # Même nombre que la classe majoritaire\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Combiner les deux classes\n",
        "oversampled_data = pd.concat([majority_class, minority_oversampled])\n",
        "\n",
        "# Répartition après oversampling\n",
        "print(\"\\nRépartition des classes après oversampling :\")\n",
        "print(oversampled_data['Class'].value_counts())\n",
        "\n",
        "# Redéfinir X_train et y_train\n",
        "X_train_oversampled = oversampled_data.iloc[:, :-1]\n",
        "y_train_oversampled = oversampled_data['Class']\n"
      ],
      "metadata": {
        "id": "vAxBXC26OYyk",
        "outputId": "66c3c291-ef43-47e2-e350-48db221ab93d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Répartition des classes après oversampling :\n",
            "Class\n",
            "0    227451\n",
            "1    227451\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train a Logistic Regression model on the original data and another Logistic Regression model on the undersampled data. We will then compare the performance of these two models to see if our undersampled data allowed better results.\n",
        "\n",
        "Evaluate the 2 models by printing the accuracy, precision, recall and F1-score metrics."
      ],
      "metadata": {
        "id": "PYEj4pyeRlmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "# Modèle sur les données originales\n",
        "model_original = LogisticRegression(max_iter=200, random_state=42)\n",
        "model_original.fit(X_train, y_train)\n",
        "y_pred_original = model_original.predict(X_test)\n",
        "\n",
        "print(\"\\nRapport de classification (données originales) :\")\n",
        "print(classification_report(y_test, y_pred_original))\n"
      ],
      "metadata": {
        "id": "PlrJp4ddPO9d",
        "outputId": "1b8649bf-fdb4-4c01-c6c0-1210aa50c1fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Rapport de classification (données originales) :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56864\n",
            "           1       0.76      0.69      0.72        98\n",
            "\n",
            "    accuracy                           1.00     56962\n",
            "   macro avg       0.88      0.85      0.86     56962\n",
            "weighted avg       1.00      1.00      1.00     56962\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modèle sur les données sur-échantillonnées\n",
        "model_oversampled = LogisticRegression(max_iter=200, random_state=42)\n",
        "model_oversampled.fit(X_train_oversampled, y_train_oversampled)\n",
        "y_pred_oversampled = model_oversampled.predict(X_test)\n",
        "\n",
        "print(\"\\nRapport de classification (données sur-échantillonnées) :\")\n",
        "print(classification_report(y_test, y_pred_oversampled))\n"
      ],
      "metadata": {
        "id": "2r5qEk5EPREd",
        "outputId": "2f6d5d62-8b6f-4158-f5be-b78f5fbea506",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Rapport de classification (données sur-échantillonnées) :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98     56864\n",
            "           1       0.05      0.91      0.09        98\n",
            "\n",
            "    accuracy                           0.97     56962\n",
            "   macro avg       0.52      0.94      0.54     56962\n",
            "weighted avg       1.00      0.97      0.98     56962\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a quick analysis of the performance metrics of both undersampled and oversampled data (you should get approximatively the same conclusions)."
      ],
      "metadata": {
        "id": "6exo9r1QRxBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Données originales :\n",
        "Bonne précision globale, mais faible rappel pour la classe fraude.\n",
        "Peu adapté à un système où la détection des fraudes est essentielle.\n",
        "\n",
        "En détails:\n",
        "\n",
        "- Classe 0 (légitime) :\n",
        "\n",
        " - Précision et rappel parfaits : le modèle est fortement biaisé en faveur de la classe majoritaire.\n",
        "\n",
        "\n",
        "- Classe 1 (fraude) :\n",
        " - Le rappel est 0.69, ce qui signifie que 31 % des fraudes ne sont pas détectées.\n",
        "\n",
        " - La précision de 0.76 montre que des transactions légitimes sont mal classées comme frauduleuses.\n",
        "\n",
        "### Données sur-échantillonnées :\n",
        "Excellente capacité à détecter les fraudes (rappel élevé), mais beaucoup de faux positifs.\n",
        "\n",
        "Approche utile si le rappel est prioritaire\n",
        "\n",
        "En détails:\n",
        "- Classe 0 (légitime) :\n",
        " - La précision reste élevée (1.00), mais le rappel diminue légèrement (0.97).\n",
        "- Classe 1 (fraude) :\n",
        " - Rappel amélioré à 0.91, capturant presque toutes les fraudes.\n",
        " - Précision très faible (0.05), ce qui génère de nombreux faux positifs."
      ],
      "metadata": {
        "id": "DXs6Ze21R2QA"
      }
    }
  ]
}